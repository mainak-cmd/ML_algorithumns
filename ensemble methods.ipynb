{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two families of ensemble methods are usually distinguished: averaging methods and boosting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driving principle is to build several estimators independently and then to average their predictions.\n",
    "(Bagging methods, Forests of randomized trees)\n",
    "the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "it is like parrale estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bagging methods form a class of algorithms which build several instances of a estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure as making an ensemble out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flavours of Bagging methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pasting : When random subsets of the dataset are drawn as random subsets of the samples.\n",
    "\n",
    "Bagging : When random subsets of the dataset are drawn as random subsets of the samples with replacement.\n",
    "    \n",
    "Random Subspaces : When random subsets of the dataset are drawn as random subsets of the features.\n",
    "    \n",
    "Random Patches : when base estimators are built on subsets of both samples and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the \n",
    "original dataset and then aggregate their individual predictions either by voting or by averaging to form a final prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimatorobject, default=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The base estimator to fit on random subsets of the dataset\n",
    "If None, then the base estimator is a DecisionTreeClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimatorsint, default=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of base estimators in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samplesint or float, default=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of samples to draw from given dataset to train each base estimator (with or without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The number of features to draw from given dataset to train each base estimator\n",
    "(without replacement by default, see bootstrap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap : bool, default=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whether samples are drawn with replacement. If False, sampling without replacement is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_features : bool, default=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Whether features are drawn with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oob_score bool, default=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting oob_score=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useing spyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state and verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose : containing more words than necessary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = preprocessing.scale(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, stratify=y,\n",
    "                                                   random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " use base estimator as DecisionTreeClassifier.\n",
    " tuning parameter\n",
    " n_estimators,base_estimator,max_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=BaggingClassifier(base_estimator=None, #The base estimator to fit on random subsets of the dataset\n",
    "n_estimators=20, #The number of samples to draw from X to train each base estimator \n",
    "max_samples=0.32, #The number of samples (batch) to draw from X to train each base estimator 30% of sample are taken\n",
    "max_features=1.0, #The number of features to draw from X to train each base estimator all samples are taken\n",
    "bootstrap=True,  # samples are drawn with replacement\n",
    "bootstrap_features=False, #features are not drawn with replacement.\n",
    "oob_score=True, # use out-of-bag samples to estimate the generalization error\n",
    "warm_start=False, n_jobs=None, #take is as defult\n",
    "random_state=1, #Controls the random resampling of the original dataset\n",
    "verbose=0 #Controls the verbosity when fitting and predicting\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(max_samples=0.32, n_estimators=20, oob_score=True,\n",
       "                  random_state=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class of an input sample is computed as the class with the highest mean predicted probability and  it resorts to voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score( X_test, y_test) # mean accuracy on the given test data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.)Stacked generalization is a method for combining estimators to reduce their biases.\n",
    "\n",
    "2.)the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction.\n",
    "\n",
    "3.)This final estimator is trained through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackingClassifier /Stack of estimators with a final classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that estimators_ are fitted on the full X while final_estimator_ is trained using cross-validated predictions of the base estimators using cross_val_predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base estimators which will be stacked together.\n",
    "and it is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A classifier which will be used to combine the base estimators. The default classifier is a LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Determines the cross-validation splitting strategy used in cross_val_predict to train final_estimator.\n",
    "1.)None, to use the default 5-fold cross validation,\n",
    "2.)integer, to specify the number of folds in a (Stratified) KFold,\n",
    "3.)An object to be used as a cross-validation generator,\n",
    "4.)An iterable yielding train, test splits.\n",
    "cv is not used for model evaluation but for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Methods called for each base estimator.\n",
    "‘auto’ will try to invoke, for each estimator, 'predict_proba', 'decision_function' or 'predict' in that order.\n",
    "otherwise, one of 'predict_proba', 'decision_function' or 'predict'. \n",
    "If the method is not implemented by the estimator, it will raise an error so we chose it by defult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobsint,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When False, only the predictions of estimators will be used as training data for final_estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When True, the final_estimator is trained on the predictions as well as the original training data both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Verbosity level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackingClassifier /Stack of estimators with a final classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every parameter is same except in regression we are not use stack_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = preprocessing.scale(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, stratify=y,\n",
    "                                                   random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [\n",
    "     ('mf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "     ('skr', LinearSVC(random_state=42)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = StackingClassifier(estimators=estimators, #Base estimators which will be stacked together.\n",
    "final_estimator=None,   #The default classifier is a LogisticRegression.\n",
    "cv=None ,#None, to use the default 5-fold cross validation\n",
    "stack_method='auto', #take is defult beacouse it will try to invoke, for each estimator all methods\n",
    "n_jobs=None,\n",
    "passthrough=False,# only the predictions of estimators will be used as training data for final_estimator\n",
    "verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(estimators=[('mf',\n",
       "                                RandomForestClassifier(n_estimators=10,\n",
       "                                                       random_state=42)),\n",
       "                               ('skr', LinearSVC(random_state=42))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.score( X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = StackingClassifier(estimators=estimators, #Base estimators which will be stacked together.\n",
    "final_estimator=None,   #The default classifier is a LogisticRegression.\n",
    "cv=None ,#None, to use the default 5-fold cross validation\n",
    "stack_method='auto', #take is defult beacouse it will try to invoke, for each estimator all methods\n",
    "n_jobs=None,\n",
    "passthrough=True,# only the predictions of estimators will be used as training data for final_estimator\n",
    "verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X_train, y_train) \n",
    "clf2.predict(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score( X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forests of randomized trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "includes two averaging algorithms based on randomized decision trees:\n",
    "1)the RandomForest algorithm\n",
    "2)Extra-Trees method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Both algorithms are perturb-and-combine techniques (good for unstable method) specifically designed for trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb:create different model\n",
    "combine : create a single prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb create different models useing\n",
    "resampleing,s\n",
    "ubsampleing,\n",
    "adding noise,\n",
    "adaptively rewaiting,\n",
    "randomly choosing from the competitor split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine pradict useing voteing weighted voteing and avarageing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In random forests  each tree in the ensemble is built from a sample drawn with replacement from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the best split is found either from all input features or a random subset of size max_features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forests achieve a reduced variance by combining diverse trees, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset \n",
    "and uses averaging to improve the predictive accuracy and control over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paraameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min_weight_fraction_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The minimum  weighted fraction (the sum total of weights) of all the input samples required to be at a leaf node.\n",
    "Samples have equal weight when sample_weight is not provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# min_impurity_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " min_impurity_decrease=N_t / N * {impurity - (N_t_R / N_t * right_impurity)- (N_t_L / N_t * left_impurity)}\n",
    "where N is the total number of samples, \n",
    "N_t is the number of samples at the current node,\n",
    "N_t_L is the number of samples in the left child, \n",
    "and N_t_R is the number of samples in the right child.\n",
    "\n",
    "example\n",
    "N_t = 26\n",
    "N = 90\n",
    "N_t_R = 4\n",
    "N_t_L = 22\n",
    "impurity = 0.2041\n",
    "right impurity = 0.375\n",
    "left impurity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=26/90 #N_t / N\n",
    "impurity=0.2041\n",
    "c=4/26 #N_t_R / N_t\n",
    "d=22/90#N_t_L / N_t\n",
    "right_impurity = 0.375\n",
    "left_impurity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " min_impurity_decrease=a*(impurity-(c*right_impurity)-(d*left_impurity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042295555555555545"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " min_impurity_decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A node will be split if this split induces a decrease of the impurity 0.042295555555555545 greater than or equal to 0.0\n",
    "defult 0 to consider full length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bootstrap and  oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap\n",
    "Fix the sample size\n",
    "Randomly choose a data point for a sample\n",
    "After selection, keep it back in the main set (replacement)\n",
    "Again choose a data point from the main training set for the sample and after selection, keep it back.\n",
    "Perform the above steps, till we reach the specified sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oob_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While making the samples, data points were chosen randomly and with replacement, and the data points which fail to be a part of that particular sample are known as OUT-OF-BAG points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " OOB_Score prevents leakage and gives a better model with low variance, so we use OOB_score for validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No leakage of data,Less Variance,Less Computatio \n",
    "Better Predictive Model: for small and medium size data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "would you need to increase the number of estimators before approaching a new fit\n",
    "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1.\n",
    "So higher class-weight means you want to put more emphasis on a class.\n",
    "\n",
    "example\n",
    "From what you say it seems class 0 is 19 times more frequent than class 1. \n",
    "So you should increase the class_weight of class 1 relative to class 0, say {0:.1, 1:.9}.\n",
    "\n",
    "If the class_weight doesn't sum to 1, it will basically change the regularization parameter.\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in \n",
    "the input data as n_samples / (n_classes * np.bincount(y)).(use it for unbalanced  dataset )\n",
    "\n",
    "For multi-output, the weights of each column of y will be multiplied.\n",
    "\n",
    "If not given, all classes are supposed to have weight one (n0ne)\n",
    "\n",
    "weights associated with classes in the form {class_label: weight}\n",
    "\n",
    "For multi-output problems, a list of dicts can be provided in the same order as the columns of y and the \n",
    "weights of each column of y will be multiplied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Minimal cost complexity pruning recursively finds the node with the “weakest link”.\n",
    "As alpha increases, more of the tree is pruned, which increases the total impurity of its leaves.\n",
    "\n",
    "\n",
    "This algorithm is parameterized by α(≥0) known as the complexity parameter.\n",
    "\n",
    "The complexity parameter is used to define the cost-complexity measure, Rα(T) of a given tree T: Rα(T)=R(T)+α|T|\n",
    "\n",
    "where |T| is the number of terminal nodes \n",
    "in T and R(T) is traditionally defined as the total misclassification rate of the terminal nodes.\n",
    "\n",
    "DecisionTree in sklearn has a function called cost_complexity_pruning_path, which gives the effective alphas of \n",
    "subtrees during pruning and also the corresponding impurities. In other words, we can use these values of alpha to \n",
    "prune our decision tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1)it the data use all bydefult setting of tree.\n",
    "2)findout max depth and  ccp_alpha useing cost_complexity_pruning_path(X, y[, …])and get_depth() funtion\n",
    "3)useing grid scherch cv to pass all parameters (depth,ccp_alpha) to find out best parameter.\n",
    "4)use those parameter to RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier(n_estimators=100, #The number of trees in the forest.\n",
    "                       criterion='gini', #The function to measure the quality of a split.\n",
    "                       max_depth=None,#The maximum depth of the tree(defult to allow max_depth)\n",
    "                       min_samples_split=2, #The minimum number of samples required to split an internal node(defult)\n",
    "                       min_samples_leaf=1, #The minimum number of samples required to be at a leaf node.(for classification 1)\n",
    "                       min_weight_fraction_leaf=0.0, #Samples have equal weight when sample_weight is not provided\n",
    "                       max_features='auto', #The number of features to consider when looking for the best split\n",
    "                       max_leaf_nodes=None, #Grow trees with max_leaf_nodes in best-first fashion\n",
    "                       min_impurity_decrease=0.0, #A node will be split if the impurity decrease greater than or equal to this value.\n",
    "                       min_impurity_split=0.0, #A node will split if its impurity is above the threshold\n",
    "                       bootstrap=True, #Whether bootstrap samples are used when building trees.(always true for RandomForest )\n",
    "                       oob_score=True, #Whether to use out-of-bag samples to estimate the generalization score(unbalanced dataset)\n",
    "                       n_jobs=None, #he number of jobs to run in parallel.\n",
    "                       random_state=0, #Controls both the randomness of the bootstrapping of the samples used when building trees\n",
    "                       verbose=0, #Controls the verbosity when fitting and predicting.\n",
    "                       warm_start=False, #fit a whole new forest\n",
    "                       class_weight=None, # all classes are supposed to have weight one \n",
    "                       ccp_alpha=0.0,#Complexity parameter used for Minimal Cost-Complexity Pruning. \n",
    "                       max_samples=None#If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor(n_estimators=100, #The number of trees in the forest.\n",
    "                      criterion='mse', #The function to measure the quality of a split.\n",
    "                      max_depth=None, #The maximum depth of the tree(defult to allow max_depth)\n",
    "                      min_samples_split=2, #The minimum number of samples required to split an internal node(defult)\n",
    "                      min_samples_leaf=1,  #The minimum number of samples required to be at a leaf node.(for classification 1)\n",
    "                      min_weight_fraction_leaf=0.0,#Samples have equal weight when sample_weight is not provided\n",
    "                      max_features='auto',#The number of features to consider when looking for the best split\n",
    "                      max_leaf_nodes=None, #Grow trees with max_leaf_nodes in best-first fashion\n",
    "                      min_impurity_decrease=0.0, #A node will be split if the impurity decrease greater than or equal to this value.\n",
    "                      min_impurity_split=None, #A node will split if its impurity is above the threshold\n",
    "                      bootstrap=True, #Whether bootstrap samples are used when building trees.(always true for RandomForest )\n",
    "                      oob_score=False, #Whether to use out-of-bag samples to estimate the generalization score(unbalanced dataset)\n",
    "                      n_jobs=None, #he number of jobs to run in parallel.\n",
    "                      random_state=None, #Controls both the randomness of the bootstrapping of the samples used when building trees\n",
    "                      verbose=0, #Controls the verbosity when fitting and predicting.\n",
    "                      warm_start=False, #fit a whole new forest\n",
    "                      ccp_alpha=0.0, #Complexity parameter used for Minimal Cost-Complexity Pruning.\n",
    "                      max_samples=None#If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest chooses the optimum split while Extra Trees chooses it randomly.\n",
    "This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias\n",
    "improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtraTreesClassifier(n_estimators=100,#The number of trees in the forest.\n",
    "                     criterion='gini',\n",
    "                     max_depth=None, \n",
    "                     min_samples_split=2, \n",
    "                     min_samples_leaf=1, \n",
    "                     min_weight_fraction_leaf=0.0, \n",
    "                     max_features='auto',\n",
    "                     max_leaf_nodes=None,\n",
    "                     min_impurity_decrease=0.0, \n",
    "                     min_impurity_split=None,\n",
    "                     bootstrap=False, #the whole dataset is used to build each tree.\n",
    "                     oob_score=False, # Only available if bootstrap=True.\n",
    "                     n_jobs=None, \n",
    "                     random_state=None, \n",
    "                     verbose=0, \n",
    "                     warm_start=False, \n",
    "                     class_weight=None, \n",
    "                     ccp_alpha=0.0,\n",
    "                     max_samples=None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExtraTreesRegressor(n_estimators=100,\n",
    "                    criterion='mse',\n",
    "                    max_depth=None,\n",
    "                    min_samples_split=2, \n",
    "                    min_samples_leaf=1, \n",
    "                    min_weight_fraction_leaf=0.0, \n",
    "                    max_features='auto', \n",
    "                    max_leaf_nodes=None, \n",
    "                    min_impurity_decrease=0.0, \n",
    "                    min_impurity_split=None, \n",
    "                    bootstrap=False, \n",
    "                    oob_score=False, \n",
    "                    n_jobs=None, \n",
    "                    random_state=None, \n",
    "                    verbose=0, \n",
    "                    warm_start=False, \n",
    "                    ccp_alpha=0.0, \n",
    "                    max_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boosting methods,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator it is like seirise of estimator.\n",
    "The motivation is to combine several weak models to produce a powerful ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The core principle of AdaBoost is to fit a sequence of weak learners  on repeatedly modified versions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first step simply trains a weak learner on the original data.\n",
    "\n",
    "For each successive iteration, the sample weights(initialy 1) are individually modified \n",
    "and the learning algorithm is reapplied to the reweighted data\n",
    "    \n",
    "At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step\n",
    "have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
    "\n",
    "As iterations proceed, examples that are difficult to predict receive ever-increasing influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimato (weak learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weak Classifier: Formally, a classifier that achieves slightly better than 50 percent accuracy.(binary classification.)\n",
    "\n",
    "Weak Learner:  a model that performs slightly better than a naive model.it has been generalized to multi-class classification \n",
    "and has a different meaning beyond better than 50 percent accuracy.\n",
    "\n",
    "Decision Stump: A decision tree with a single node operating on one input variable, the output of which \n",
    "makes a prediction directly.\n",
    "\n",
    "weak learning models\n",
    "k-Nearest Neighbors, with k=1 operating on one or a subset of input variables.\n",
    "DecisionTreeClassifier initialized with max_depth=1\n",
    "Naive Bayes, operating on a single input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For multi-class classification, AdaBoostClassifier implements AdaBoost-SAMME and AdaBoost-SAMME.R \n",
    "For regression, AdaBoostRegressor implements AdaBoost.R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostClassifier(base_estimator=None, # If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\n",
    "                   n_estimators=50, #The maximum number of estimators at which boosting is terminated. \n",
    "                   learning_rate=1.0,#Weight applied to each classifier at each boosting iteration.\n",
    "                   algorithm='SAMME.R', # lower test error with fewer boosting iterations.\n",
    "                   random_state=None #Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "                \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A higher learning rate increases the contribution of each classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoostRegressor(base_estimator=None,#The base estimator from which the boosted ensemble is built. \n",
    "                  n_estimators=50, #The maximum number of estimators at which boosting is terminated\n",
    "                  learning_rate=1.0, #Weight applied to each classifier at each boosting iteration.\n",
    "                  loss='linear', #he loss function to use when updating the weights after each boosting iteration.\n",
    "                  random_state=None#Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities\n",
    "(soft vote) to predict the class labels.\n",
    "\n",
    "Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.\n",
    "\n",
    "Majority Class Labels (Majority/Hard Voting)\n",
    "\n",
    "Weighted Average Probabilities (Soft Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Majority Class Labels (Majority/Hard Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If ‘hard’, uses predicted class labels for majority rule voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode)\n",
    "of the class labels predicted by each individual classifier.\n",
    "\n",
    "if the prediction for a given sample is\n",
    "\n",
    "classifier 1 -> class 1\n",
    "\n",
    "classifier 2 -> class 1\n",
    "\n",
    "classifier 3 -> class 2\n",
    "\n",
    "the VotingClassifier (with voting='hard') would classify the sample as “class 1” based on the majority class \n",
    "Sequence of weights (float or int) to weight the occurrences of predicted class labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If ‘hard’, uses predicted class labels for majority rule voting.\n",
    "\n",
    "Weighted Average Probabilities (Soft Voting)\n",
    "oft voting returns the class label as argmax of the sum of predicted probabilitie\n",
    "The weighted average probabilities for a sample would then be calculated as follows:\n",
    "\n",
    "which is recommended for an ensemble of well-calibrated classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence of weights to weight the occurrences of predicted class labels \n",
    "class probabilities before averaging (soft voting).\n",
    "\n",
    "array-like of shape (n_classifiers,)\n",
    "class probabilities before averaging\n",
    "Number of `estimators` and weights must be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting='hard'\n",
    "VotingClassifier(estimators, \n",
    "                 voting='hard', #If ‘hard’, uses predicted class labels for majority rule voting\n",
    "                 weights=None, #Uses uniform weights for hard voteing\n",
    "                 n_jobs=None, #The number of jobs to run in parallel for fit. \n",
    "                 flatten_transform=False, #Affects shape of transform output it returns (n_classifiers, n_samples, n_classes).\n",
    "                 verbose=False\n",
    "                )\n",
    "\n",
    "#voting='soft'\n",
    "VotingClassifier(estimators, \n",
    "                 voting='soft', #‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities,\n",
    "                 weights=None, #Uses uniform weights for hard voteing,Sequence of weights to weight the occurrences of predicted\n",
    "                 n_jobs=None, #The number of jobs to run in parallel for fit. \n",
    "                 flatten_transform=True, #Affects shape of transform output it returns (n_classifiers, n_samples, n_classes).\n",
    "                 verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X=pd.DataFrame(iris.data)\n",
    "y=pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                   random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = y_test[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    13\n",
       "1    13\n",
       "0    12\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useing parameter for VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights=None #beacouse all class has same frequency\n",
    "flatten_transform=False ##Affects shape of transform output\n",
    "voting='hard' #uses predicted class labels for majority rule voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GaussianNB\n",
    "gnb=GaussianNB(priors=None,#Prior probabilities of the classes ,calculated by defult\n",
    "               var_smoothing=1e-09 #Portion of the largest variance \n",
    "              )\n",
    "\n",
    "\n",
    "#RandomForestClassifie\n",
    "Ran=RandomForestClassifier(n_estimators=100, #The number of trees in the forest.\n",
    "                       criterion='gini', #The function to measure the quality of a split.\n",
    "                       max_depth=None,#The maximum depth of the tree(defult to allow max_depth)\n",
    "                       min_samples_split=2, #The minimum number of samples required to split an internal node(defult)\n",
    "                       min_samples_leaf=1, #The minimum number of samples required to be at a leaf node.(for classification 1)\n",
    "                       min_weight_fraction_leaf=0.0, #Samples have equal weight when sample_weight is not provided\n",
    "                       max_features='auto', #The number of features to consider when looking for the best split\n",
    "                       max_leaf_nodes=None, #Grow trees with max_leaf_nodes in best-first fashion\n",
    "                       min_impurity_decrease=0.0, #A node will be split if the impurity decrease greater than or equal to this value.\n",
    "                       min_impurity_split=0.0, #A node will split if its impurity is above the threshold\n",
    "                       bootstrap=True, #Whether bootstrap samples are used when building trees.(always true for RandomForest )\n",
    "                       oob_score=True, #Whether to use out-of-bag samples to estimate the generalization score(unbalanced dataset)\n",
    "                       n_jobs=None, #he number of jobs to run in parallel.\n",
    "                       random_state=0, #Controls both the randomness of the bootstrapping of the samples used when building trees\n",
    "                       verbose=0, #Controls the verbosity when fitting and predicting.\n",
    "                       warm_start=False, #fit a whole new forest\n",
    "                       class_weight=None, # all classes are supposed to have weight one \n",
    "                       ccp_alpha=0.0,#Complexity parameter used for Minimal Cost-Complexity Pruning. \n",
    "                       max_samples=None#If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "                      )\n",
    "\n",
    "#Logistic regression\n",
    "log= LogisticRegression(penalty='l2',# norm used in the penalization\n",
    "                       dual=False, # when n_samples > n_features.\n",
    "                       tol=0.0001, #The tolerance for the optimization\n",
    "                       fit_intercept=True,#data is not expected to be centered\n",
    "                       class_weight=None,#Weights associated with classes but not consider in logit\n",
    "                       random_state=0,\n",
    "                       max_iter=1000, #Maximum number of iterations taken for the solvers to converge\n",
    "                       multi_class='multinomial',#muliclass \n",
    "                       verbose=0, warm_start=False, n_jobs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators1=[('log', log), ('Ran', Ran), ('gnb', gnb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1=VotingClassifier(estimators=estimators1, \n",
    "                 voting='hard', #If ‘hard’, uses predicted class labels for majority rule voting\n",
    "                 weights=None, #Uses uniform weights for hard voteing\n",
    "                 n_jobs=None, #The number of jobs to run in parallel for fit. \n",
    "                 flatten_transform=False, #Affects shape of transform output it returns (n_classifiers, n_samples, n_classes).\n",
    "                 verbose=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n",
      "D:\\m y python\\enviourment\\anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:306: FutureWarning: The min_impurity_split parameter is deprecated. Its default value has changed from 1e-7 to 0 in version 0.23, and it will be removed in 0.25. Use the min_impurity_decrease parameter instead.\n",
      "  warnings.warn(\"The min_impurity_split parameter is deprecated. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log',\n",
       "                              LogisticRegression(max_iter=1000,\n",
       "                                                 multi_class='multinomial',\n",
       "                                                 random_state=0)),\n",
       "                             ('Ran',\n",
       "                              RandomForestClassifier(min_impurity_split=0.0,\n",
       "                                                     oob_score=True,\n",
       "                                                     random_state=0)),\n",
       "                             ('gnb', GaussianNB())],\n",
       "                 flatten_transform=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821428571428571"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1.score(X_train, y_train) # mean accuracy on the given test data and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2,\n",
       "       2, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 0, 0, 1, 2, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V1.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VotingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine conceptually different machine learning regressors and return the average predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sequence of weights (float or int) to weight the occurrences of predicted values before averaging. \n",
    "Uses uniform weights if None.\n",
    "Number of `estimators` and weights must be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VotingRegressor(estimators, \n",
    "                weights=None, #uniform weights if None.\n",
    "                n_jobs=None,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exmple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weak learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K_neigh_r = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "reg=LinearRegression( fit_intercept=True,#calculate the intercept for this model\n",
    "                     normalize=True,#the regressors X will be normalized before regressin \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost_1=AdaBoostRegressor(base_estimator=K_neigh_r,#The base estimator from which the boosted ensemble is built. \n",
    "                  n_estimators=50, #The maximum number of estimators at which boosting is terminated\n",
    "                  learning_rate=1.0, #Weight applied to each classifier at each boosting iteration.\n",
    "                  loss='linear', #he loss function to use when updating the weights after each boosting iteration.\n",
    "                  random_state=None#Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "                 )\n",
    "\n",
    "ada_boost_2=AdaBoostRegressor(base_estimator=reg,#The base estimator from which the boosted ensemble is built. \n",
    "                  n_estimators=50, #The maximum number of estimators at which boosting is terminated\n",
    "                  learning_rate=1.0, #Weight applied to each classifier at each boosting iteration.\n",
    "                  loss='linear', #he loss function to use when updating the weights after each boosting iteration.\n",
    "                  random_state=None#Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "                 )\n",
    "\n",
    "\n",
    "ada_boost_3=AdaBoostRegressor(base_estimator=None,# If None, then the base estimator is DecisionTreeRegressor \n",
    "                  n_estimators=50, #The maximum number of estimators at which boosting is terminated\n",
    "                  learning_rate=1.0, #Weight applied to each classifier at each boosting iteration.\n",
    "                  loss='linear', #he loss function to use when updating the weights after each boosting iteration.\n",
    "                  random_state=None#Controls the random seed given at each base_estimator at each boosting iteration.\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators2=[('ada_boost_1', ada_boost_1), ('ada_boost_2', ada_boost_2), ('ada_boost_3', ada_boost_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_r=VotingRegressor(estimators=estimators2,weights= [0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da1= load_boston()\n",
    "X=da1.data\n",
    "y=da1.target\n",
    "scaled_X = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4=V_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_redict=model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120136463046514"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.score(X_train, y_train, sample_weight=None) #Return the coefficient of determination r2 of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8345345410159364"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "r2_score(y_test,y_redict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both bais and vvarience are good enough to use the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
